
@incollection{Tan1993,
abstract = {Intelligent human agents exist in a cooperative social environment that facilitates learning. They learn not only by trialand -error, but also through cooperation by sharing instantaneous information, episodic experience, and learned knowledge. The key investigations of this paper are, "Given the same number of reinforcement learning agents, will cooperative agents outperform independent agents who do not communicate during learning?" and "What is the price for such cooperation?" Using independent agents as a benchmark, cooperative agents are studied in following ways: (1) sharing sensation, (2) sharing episodes, and (3) sharing learned policies. This paper shows that (a) additional sensation from another agent is beneficial if it can be used efficiently, (b) sharing learned policies or episodes among agents speeds up learning at the cost of communication, and (c) for joint tasks, agents engaging in partnership can significantly outperform independent agents although they may learn slowly in the beginning. These tradeoffs are not just limited to multi-agent reinforcement learning},
author = {Tan, Ming},
booktitle = {Machine Learning Proceedings 1993},
doi = {10.1016/B978-1-55860-307-3.50049-6},
isbn = {1-55860-495-2},
issn = {13872532},
pages = {330--337},
pmid = {14561325},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603073500496},
year = {1993}
}
@article{Mousavi2017,
abstract = {{\textcopyright} The Institution of Engineering and Technology 2017 Recent advances in combining deep neural network architectures with reinforcement learning (RL) techniques have shown promising potential results in solving complex control problems with high-dimensional state and action spaces. Inspired by these successes, in this study, the authors built two kinds of RL algorithms: deep policy-gradient (PG) and value-function-based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The PG-based agent maps its observation directly to the control signal; however, the value-function-based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Their methods show promising results in a traffic network simulated in the simulation of urban mobility traffic simulator, without suffering from instability issues during the training process.},
author = {Mousavi, S.S. and Schukat, M. and Howley, E.},
doi = {10.1049/iet-its.2017.0153},
issn = {1751956X},
journal = {IET Intelligent Transport Systems},
number = {7},
title = {{Traffic light control using deep policy-gradient and value-function-based reinforcement learning}},
volume = {11},
year = {2017}
}
@article{Karlaftis2011,
abstract = {In the field of transportation, data analysis is probably the most important and widely used research tool available. In the data analysis universe, there are two 'schools of thought'; the first uses statistics as the tool of choice, while the second - one of the many methods from - Computational Intelligence. Although the goal of both approaches is the same, the two have kept each other at arm's length. Researchers frequently fail to communicate and even understand each other's work. In this paper, we discuss differences and similarities between these two approaches, we review relevant literature and attempt to provide a set of insights for selecting the appropriate approach. {\textcopyright} 2010 Elsevier Ltd.},
author = {Karlaftis, M. G. and Vlahogianni, E. I.},
doi = {10.1016/j.trc.2010.10.004},
isbn = {0968-090X},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
keywords = {Neural networks,Statistical models,Transportation research},
number = {3},
pages = {387--399},
title = {{Statistical methods versus neural networks in transportation research: Differences, similarities and some insights}},
volume = {19},
year = {2011}
}
@inproceedings{El-Tantawy2012,
abstract = {Population is steadily increasing worldwide, resulting in intractable traffic congestion in dense urban areas. Adaptive traffic signal control ({\{}ATSC){\}} has shown strong potential to effectively alleviate urban traffic congestion by adjusting signal timing plans in real time in response to traffic fluctuations to achieve desirable objectives (e.g., minimize delay). Efficient and robust {\{}ATSC{\}} can be designed using a multiagent reinforcement learning ({\{}MARL){\}} approach in which each controller (agent) is responsible for the control of traffic lights around a single traffic junction. Applying {\{}MARL{\}} approaches to the {\{}ATSC{\}} problem is associated with a few challenges as agents typically react to changes in the environment at the individual level, but the overall behavior of all agents may not be optimal. This paper presents the development and evaluation of a novel system of multiagent reinforcement learning for integrated network of adaptive traffic signal controllers ({\{}MARLIN-ATSC).{\}} {\{}MARLIN-ATSC{\}} offers two possible modes: 1) independent mode, where each intersection controller works independently of other agents; and 2) integrated mode, where each controller coordinates signal control actions with neighboring intersections. {\{}MARLIN-ATSC{\}} is tested on a large-scale simulated network of 59 intersections in the lower downtown core of the City of Toronto, {\{}ON{\}}, Canada, for the morning rush hour. The results show unprecedented reduction in the average intersection delay ranging from 27{\%} in mode 1 to 39{\%} in mode 2 at the network level and travel-time savings of 15{\%} in mode 1 and 26{\%} in mode 2, along the busiest routes in Downtown Toronto.},
author = {El-Tantawy, Samah and Abdulhai, Baher},
booktitle = {2012 15th International IEEE Conference on Intelligent Transportation Systems},
doi = {10.1109/ITSC.2012.6338707},
isbn = {978-1-4673-3063-3},
issn = {15249050},
pages = {319--326},
title = {{Multi-Agent Reinforcement Learning for Integrated Network of Adaptive Traffic Signal Controllers (MARLIN-ATSC)}},
year = {2012}
}
@article{DBLP:journals/corr/FoersterAFW16a,
archivePrefix = {arXiv},
arxivId = {1605.06676},
author = {Foerster, Jakob N and Assael, Yannis M and de Freitas, Nando and Whiteson, Shimon},
eprint = {1605.06676},
journal = {CoRR},
title = {{Learning to Communicate with Deep Multi-Agent Reinforcement Learning}},
url = {http://arxiv.org/abs/1605.06676},
volume = {abs/1605.0},
year = {2016}
}
@article{Pynadath2002,
abstract = {Despite the significant progress in multiagent teamwork, existing research does not address the optimality of its prescriptions nor the complexity of the teamwork problem. Without a characterization of the optimality-complexity tradeoffs, it is impossible to determine whether the assumptions and approximations made by a particular theory gain enough efficiency to justify the losses in overall performance. To provide a tool for use by multiagent researchers in evaluating this tradeoff, we present a unified framework, the COMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model combines and extends existing multiagent theories, such as decentralized partially observable Markov decision processes and economic team theory. In addition to their generality of representation, COM-MTDPs also support the analysis of both the optimality of team performance and the computational complexity of the agents' decision problem. In analyzing complexity, we present a breakdown of the computational complexity of constructing optimal teams under various classes of problem domains, along the dimensions of observability and communication cost. In analyzing optimality, we exploit the COM-MTDP's ability to encode existing teamwork theories and models to encode two instantiations of joint intentions theory taken from the literature. Furthermore, the COM-MTDP model provides a basis for the development of novel team coordination algorithms. We derive a domain-independent criterion for optimal communication and provide a comparative analysis of the two joint intentions instantiations with respect to this optimal policy. We have implemented a reusable, domain-independent software package based on COM-MTDPs to analyze teamwork coordination strategies, and we demonstrate its use by encoding and evaluating the two joint intentions strategies within an example domain.},
archivePrefix = {arXiv},
arxivId = {1106.4569},
author = {Pynadath, David V. and Tambe, Milind},
doi = {10.1613/jair.1024},
eprint = {1106.4569},
isbn = {10769757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {389--423},
title = {{The communicative multiagent team decision problem: Analyzing teamwork theories and models}},
volume = {16},
year = {2002}
}
@article{DBLP:journals/corr/HausknechtS15,
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew J and Stone, Peter},
eprint = {1507.06527},
journal = {CoRR},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
volume = {abs/1507.0},
year = {2015}
}
@inproceedings{Chu2017,
abstract = {Reinforcement learning (RL) is a promising technique for adaptive signal control based on real-time traffic data. However, the implementation of RL in large- scale traffic signal network remains an open challenge due to the extremely high computational cost. In this paper, we integrate a distributed communication technique into RL so that the learning and searching cost of large- scale multi-agent system can be significantly reduced. In particular, we first decompose the global Q-function into local Q-functions, so each local signal agent can compute its own optimal policy based on local observations. We then apply the max-sum message passing algorithm to share information among agents, for finding a stable and optimal global policy that is accepted by all agents. Due to this information sharing characteristic, our approach is more optimal than decentralized RL, and it is much more efficient than centralized RL. Further, we perform realistic simulation to demonstrate that this distributed RL approach outperforms decentralized RL algorithm, as well as some typical heuristic decentralized control methods.},
author = {Chu, Tianshu and Wang, Jie},
booktitle = {2017 American Control Conference (ACC)},
doi = {10.23919/ACC.2017.7963745},
isbn = {978-1-5090-5992-8},
keywords = {Aerospace electronics,Approximation algorithms,Feature extraction,Learning (artificial intelligence),Optimal control,RL,Real-time systems,Roads,adaptive control,adaptive signal control,decentralised control,distributed RL approach,distributed communication technique,distributed reinforcement learning,global Q-function,intelligent control,large-scale traffic signal network,learning (artificial intelligence),learning cost,local Q-functions,local observations,local signal agent,max-sum message passing algorithm,min-sum communication,multi-agent systems,optimal global policy,optimal policy,real-time traffic data,road traffic,searching cost,traffic signal control,typical heuristic decentralized control methods},
pages = {5095--5100},
title = {{Traffic signal control by distributed Reinforcement Learning with min-sum communication}},
url = {http://ieeexplore.ieee.org/document/7963745/},
year = {2017}
}
@article{SUMO2012,
abstract = {SUMO is an open source traffic simulation package including the simulation
application itself as well as supporting tools, mainly for network import
and demand modeling. SUMO helps to investigate a large variety of research topics,
mainly in the context of traffic management and vehicular communications.
We describe the current state of the package, its major applications,
both by research topic and by example, as well as future developments and extensions.},
author = {Krajzewicz, Daniel and Erdmann, Jakob and Behrisch, Michael and Bieker, Laura},
journal = {International Journal On Advances in Systems and Measurements},
keywords = {microscopic traffic simulation,open source,traffic management},
month = {dec},
number = {3{\&}4},
pages = {128--138},
series = {International Journal On Advances in Systems and Measurements},
title = {{Recent Development and Applications of {\{}SUMO - Simulation of Urban MObility{\}}}},
url = {http://elib.dlr.de/80483/},
volume = {5},
year = {2012}
}
@article{Foerster2017,
archivePrefix = {arXiv},
arxivId = {1702.08887},
author = {Foerster, Jakob N and Nardelli, Nantas and Farquhar, Gregory and Torr, Philip H S and Kohli, Pushmeet and Whiteson, Shimon},
eprint = {1702.08887},
journal = {CoRR},
title = {{Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.08887},
volume = {abs/1702.0},
year = {2017}
}
@article{Mnih2015a,
abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. It therefore provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as integer addition and determining the parity of random binary vectors. It is able to solve these problems for 15-digit integers and 250-bit vectors respectively. We then give results for three empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. We also observe that a two-dimensional translation model based on Grid LSTM outperforms a phrase-based reference system on a Chinese-to-English translation task, and that 3D Grid LSTM yields a near state-of-the-art error rate of 0.32{\%} on MNIST.},
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis and van Hasselt, Hado and Guez, Arthur and Silver, David and Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia and Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis and Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David and Rusu, Andrei A and {Gomez Colmenarejo}, Sergio and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia and Radford, Alec and Metz, Luke and Chintala, Soumith and {\'{O}}lafsd{\'{o}}ttir, H Freyja and Barry, Caswell and Saleem, Aman B and Hassabis, Demis and Spiers, Hugo J and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin and Le, Quoc V and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S and Dean, Jeff and Ng, Andrew Y and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis and {\'{O}}lafsd{\'{o}}ttir, H Freyja and Barry, Caswell and Saleem, Aman B and Hassabis, Demis and Spiers, Hugo J and Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David and Rusu, Andrei A and {Gomez Colmenarejo}, Sergio and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia and van Hasselt, Hado and Guez, Arthur and Silver, David and Gregor, Karol and Danihelka, Ivo and Graves, Alex and {Jimenez Rezende}, Danilo and Wierstra, Daan and Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex and Lange, S. and Gabel, T. and Riedmiller, Martin and Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex and Gregor, Karol and Danihelka, Ivo and Graves, Alex and {Jimenez Rezende}, Danilo and Wierstra, Daan},
doi = {10.1038/nature14236},
eprint = {1502.04623},
isbn = {978-1-4799-0356-6},
issn = {0028-0836},
journal = {International Conference in Machine Learning},
keywords = {deep learning,unsupervised learning},
number = {7540},
pages = {14},
pmid = {25719670},
title = {{Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1507.01526{\%}5Cnhttp://dx.doi.org/10.1007/978-3-642-27645-3{\_}2{\%}5Cnhttp://arxiv.org/abs/1112.6209{\%}5Cnhttp://arxiv.org/abs/1509.06461{\%}5Cnhttp://www.arxiv.org/pdf/1509.06461.pdf{\%}5Cnhttp://arxiv.org/abs/1511.06295{\%}5Cnhttp://arxiv.org/abs/151},
volume = {4},
year = {2015}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/BF00992698},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
url = {http://link.springer.com/10.1007/BF00992698},
volume = {8},
year = {1992}
}
@book{Goodfellow-et-al-2016,
annote = {$\backslash$url{\{}http://www.deeplearningbook.org{\}}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@article{Dusparic2016,
abstract = {This article describes Distributed W-Learning (DWL), a reinforcement learning-based algorithm for collaborative agent-based optimization of pervasive systems. DWL supports optimization towards multiple heterogeneous policies and addresses the challenges arising from the heterogeneity of the agents that are charged with implementing them. DWL learns and exploits the dependencies between agents and between policies to improve overall system performance. Instead of always executing the locally-best action, agents learn how their actions affect their immediate neighbors and execute actions suggested by neighboring agents if their importance exceeds the local action's importance when scaled using a predefined or learned collaboration coefficient. We have evaluated DWL in a simulation of an Urban Traffic Control (UTC) system, a canonical example of the large-scale pervasive systems that we are addressing. We show that DWL outperforms widely deployed fixed-time and simple adaptive UTC controllers under a variety of traffic loads and patterns. Our results also confirm that enabling collaboration between agents is beneficial as is the ability for agents to learn the degree to which it is appropriate for them to collaborate. These results suggest that DWL is a suitable basis for optimization in other large-scale systems with similar characteristics.},
author = {Dusparic, Ivana and Cahill, Vinny},
doi = {10.1145/2168260.2168271},
isbn = {1556-4665},
issn = {15564665},
journal = {ACM Transactions on Autonomous and Adaptive Systems},
title = {{Autonomic multi-policy optimization in pervasive systems}},
year = {2012}
}
@inproceedings{Hasselt:2016:DRL:3016100.3016191,
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {2094--2100},
publisher = {AAAI Press},
series = {AAAI'16},
title = {{Deep Reinforcement Learning with Double Q-Learning}},
url = {http://dl.acm.org/citation.cfm?id=3016100.3016191},
year = {2016}
}
@article{Gao2017,
abstract = {Adaptive traffic signal control, which adjusts traffic signal timing according to real-time traffic, has been shown to be an effective method to reduce traffic congestion. Available works on adaptive traffic signal control make responsive traffic signal control decisions based on human-crafted features (e.g. vehicle queue length). However, human-crafted features are abstractions of raw traffic data (e.g., position and speed of vehicles), which ignore some useful traffic information and lead to suboptimal traffic signal controls. In this paper, we propose a deep reinforcement learning algorithm that automatically extracts all useful features (machine-crafted features) from raw real-time traffic data and learns the optimal policy for adaptive traffic signal control. To improve algorithm stability, we adopt experience replay and target network mechanisms. Simulation results show that our algorithm reduces vehicle delay by up to 47{\%} and 86{\%} when compared to another two popular traffic signal control algorithms, longest queue first algorithm and fixed time control algorithm, respectively.},
archivePrefix = {arXiv},
arxivId = {1705.02755},
author = {Gao, Juntao and Shen, Yulong and Liu, Jia and Ito, Minoru and Shiratori, Norio},
eprint = {1705.02755},
journal = {arXiv},
keywords = {()},
pages = {1--10},
title = {{Adaptive Traffic Signal Control: Deep Reinforcement Learning Algorithm with Experience Replay and Target Network}},
url = {https://arxiv.org/pdf/1705.02755.pdf{\%}0Ahttp://arxiv.org/abs/1705.02755},
year = {2017}
}
@article{Li2016,
abstract = {In this paper, we propose a set of algorithms to design signal timing plans via deep reinforcement learning. The core idea of this approach is to set up a deep neural network (DNN) to learn the Q-function of reinforcement learning from the sampled traffic state/control inputs and the corresponding traffic system performance output. Based on the obtained DNN, we can find the appropriate signal timing policies by implicitly modeling the control actions and the change of system states. We explain the possible benefits and implementation tricks of this new approach. The relationships between this new approach and some existing approaches are also carefully discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.09851v1},
author = {Li, Li and Lv, Yisheng and Wang, Fei-Yue},
doi = {10.1109/JAS.2016.7508798},
eprint = {arXiv:1705.09851v1},
isbn = {9781509029273},
issn = {2329-9266},
journal = {IEEE/CAA Journal of Automatica Sinica},
keywords = {Analytical models,Learning (artificial intelligence),Machine learning,Mathematical model,Neural networks,Optimization,Timing,Traffic control,deep learning,deep reinforcement learning,reinforcement learning},
number = {3},
pages = {247--254},
title = {{Traffic signal timing via deep reinforcement learning}},
url = {https://arxiv.org/pdf/1705.09851.pdf{\%}0Ahttp://ieeexplore.ieee.org/document/7508798/},
volume = {3},
year = {2016}
}
@article{DuchiJohnandHazanEladandSinger2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {{Duchi, John and Hazan, Elad and Singer}, Yoram},
journal = {The Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
number = {1532-4435},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
volume = {12},
year = {2011}
}
@article{Wang2016,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning inspired by advantage learning. Our dueling architecture represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.},
archivePrefix = {arXiv},
arxivId = {1511.06581},
author = {Wang, Ziyu and de Freitas, Nando and Lanctot, Marc},
doi = {10.1109/MCOM.2016.7378425},
eprint = {1511.06581},
isbn = {9781510829008},
issn = {0163-6804},
journal = {arXiv},
number = {9},
pages = {1--16},
pmid = {15003161},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.06581},
year = {2016}
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple com-putational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previ-ous " parameter server " designs the management of shared state is built into the system, TensorFlow enables develop-ers to experiment with novel optimizations and training al-gorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural net-works. Several Google services use TensorFlow in pro-duction, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Brain, Google},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16)},
doi = {10.1038/nn.3331},
eprint = {1605.08695},
isbn = {978-1-931971-33-1},
issn = {0270-6474},
pages = {265--284},
pmid = {16411492},
title = {{TensorFlow: A System for Large-Scale Machine Learning TensorFlow: A system for large-scale machine learning}},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
year = {2016}
}
@article{DBLP:journals/corr/abs-1710-02298,
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Daniel and Piot, Bilal and Azar, Mohammad Gheshlaghi and Silver, David},
eprint = {1710.02298},
journal = {CoRR},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1710.02298},
volume = {abs/1710.0},
year = {2017}
}
@article{Schulman2015,
abstract = {SmB6 has been predicted to be a prototype of topological Kondo insulator (TKI) but its direct experimental evidence as a TKI is still lacking to date. Here we report on our search for the signature of a topological surface state and investigation of the effect of disorder on transport properties in nanocrystalline SmB6 thin films through longitudinal magnetoresistance and Hall coefficient measurements. The magnetoresistance (MR) at 2 K is positive and linear (LPMR) at low field and become negative and quadratic at higher field. While the negative part is understood from the reduction of the hybridization gap due to Zeeman splitting, the positive dependence is similar to what is observed in other topological insulators (TI). We conclude that the LPMR is a characteristic of TI and is related to the linear dispersion near the Dirac cone. The Hall resistance shows a sign change around 50K. It peaks and becomes nonlinear around 10 K then decreases below 10 K. This indicates that carriers with opposite signs emerge below 50 K. These properties indicate that the surface states are robust and probably topological in our nanocrystalline films.},
archivePrefix = {arXiv},
arxivId = {1502.0547},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abeel, Pieter},
doi = {10.1063/1.4927398},
eprint = {1502.0547},
isbn = {0375-9687},
issn = {2158-3226},
journal = {ICML},
title = {{Trust region policy optimisation}},
year = {2015}
}
@article{Belletti2018,
abstract = {This article shows how the recent breakthroughs in Reinforcement Learning (RL) that have enabled robots to learn to play arcade video games, walk or assemble colored bricks, can be used to perform other tasks that are currently at the core of engineering cyberphysical systems. We present the first use of RL for the control of systems modeled by discretized non-linear Partial Differential Equations (PDEs) and devise a novel algorithm to use non-parametric control techniques for large multi-agent systems. We show how neural network based RL enables the control of discretized PDEs whose parameters are unknown, random, and time-varying. We introduce an algorithm of Mutual Weight Regularization (MWR) which alleviates the curse of dimensionality of multi-agent control schemes by sharing experience between agents while giving each agent the opportunity to specialize its action policy so as to tailor it to the local parameters of the part of the system it is located in.},
archivePrefix = {arXiv},
arxivId = {1701.08832},
author = {Belletti, Francois and Haziza, Daniel and Gomes, Gabriel and Bayen, Alexandre M.},
doi = {10.1109/TITS.2017.2725912},
eprint = {1701.08832},
isbn = {1524-9050 VO  - PP},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Deep learning,continuous control,deep reinforcement learning,macroscopic traffic models,partial differential equations,reinforcement learning,transportation systems},
number = {4},
pages = {1198--1207},
title = {{Expert Level Control of Ramp Metering Based on Multi-Task Deep Reinforcement Learning}},
volume = {19},
year = {2018}
}
@misc{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
booktitle = {IEEE Signal Processing Magazine},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
isbn = {9781424469178},
issn = {10535888},
number = {6},
pages = {26--38},
pmid = {25719670},
title = {{Deep reinforcement learning: A brief survey}},
volume = {34},
year = {2017}
}
@article{DBLP:journals/corr/Ruder16,
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
journal = {CoRR},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
volume = {abs/1609.0},
year = {2016}
}
@article{Hasselt2010,
abstract = {In some stochastic environments the well-known reinforcement learning algo-rithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alter-native way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes under-estimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning per-forms poorly due to its overestimation.},
author = {Hasselt, Hado Van and Group, Adaptive Computation and Wiskunde, Centrum},
isbn = {9781617823800},
journal = {Nips},
pages = {1--9},
title = {{Double Q-learning}},
year = {2010}
}
@article{VanDerPol2016,
abstract = {This paper investigates learning control policies for traffic lights. We introduce a new reward function for the traffic light control problem, and propose the combina-tion of the popular Deep Q-learning algorithm with a coordination algorithm for a scalable approach to controlling coordinating traffic lights, without requiring the simplifying assumptions made in earlier work. We show that this approach reduces travel times compared to earlier work on reinforcement learning methods for traffic light control and investigate possible causes of instability in the single-agent case.},
author = {{Van Der Pol}, Elise and Oliehoek, Frans A},
journal = {NIPS'16 Workshop on Learning, Inference and Control of Multi-Agent Systems},
number = {Nips},
title = {{Coordinated Deep Reinforcement Learners for Traffic Light Control}},
url = {http://www.fransoliehoek.net/docs/VanDerPol16LICMAS.pdf{\%}0Ahttp://elisevanderpol.nl/papers/vanderpol{\_}oliehoek{\_}nipsmalic2016.pdf},
year = {2016}
}
@article{Rosenschein1994,
abstract = {As distributed systems of computers play an increasingly important$\backslash$nrole in society, it will be necessary to consider ways in which these$\backslash$nmachines can be made to interact effectively. We are concerned with$\backslash$nheterogeneous, distributed systems made up of machines that have$\backslash$nbeen programmed by different entities to pursue different goals.$\backslash$nAdjusting the rules of public behavior (the rules of the game) by$\backslash$nwhich the programs must interact can influence the private strategies$\backslash$nthat designers set up in their machines. These rules can shape the$\backslash$ndesign choices of the machines' programmers and, thus, the run-time$\backslash$nbehavior of their creations. Certain kinds of desirable social behavior$\backslash$ncan thus be caused to emerge through the careful design of interaction$\backslash$nrules. Formal tools and analysis can help in the appropriate design$\backslash$nof these rules. We consider how concepts from fields such as decision$\backslash$ntheory and game theory can provide standards to be used in the design$\backslash$nof appropriate negotiation and interaction environments. This design$\backslash$nis highly sensitive to the domain in which the interaction is taking$\backslash$nplace. This article is adapted from an invited lecture given by Jeffrey$\backslash$nRosenschein at the Thirteenth International Joint Conference on Artificial$\backslash$nIntelligence in Chambery, France, on 2 September 1993.},
author = {Rosenschein, Js and Zlotkin, Gilad},
isbn = {0262181592},
issn = {0738-4602},
journal = {AI magazine},
keywords = {agent,automation,negotiation},
number = {3},
pages = {29--46},
title = {{Designing conventions for automated negotiation}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/1098},
volume = {15},
year = {1994}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
journal = {International Conference on Learning Representations 2015},
pages = {1--15},
pmid = {172668},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@article{Gupta2017,
abstract = {This work considers the problem of learning cooperative policies in complex, partially observable domains without explicit communi-cation. We extend three classes of single-agent deep reinforcement learning algorithms based on policy gradient, temporal-difference error, and actor-critic methods to cooperative multi-agent systems. We introduce a set of cooperative control tasks that includes tasks with discrete and continuous actions, as well as tasks that involve hundreds of agents. The three approaches are evaluated against each other using different neural architectures, training procedures, and reward structures. Using deep reinforcement learning with a curriculum learning scheme, our approach can solve problems that were previously considered intractable by most multi-agent rein-forcement learning algorithms. We show that policy gradient meth-ods tend to outperform both temporal-difference and actor-critic methods when using feed-forward neural architectures. We also show that recurrent policies, while more difficult to train, outper-form feed-forward policies on our evaluation tasks.},
author = {Gupta, Jayesh K and Egorov, Maxim and Kochenderfer, Mykel},
doi = {10.1007/978-3-319-71682-4},
isbn = {9783319716824},
journal = {Adaptive Learning Agents (ALA) 2017},
title = {{Cooperative Multi-Agent Control Using Deep Reinforcement Learning}},
url = {http://ala2017.it.nuigalway.ie/papers/ALA2017{\_}Gupta.pdf},
year = {2017}
}
@article{Chu2014,
abstract = {— Reinforcement learning in a large-scale sys-tem is computationally challenging due to the curse of the dimensionality. One approach is to approximate the Q-function as a function of a state-action related fea-ture vector, then learn the parameters instead. Although assumptions from the priori knowledge can potentially explore an appropriate feature vector, selecting a biased one that insufficiently represents the system usually leads to the poor learning performance. To avoid this disadvantage, this paper introduces kernel methods to implicitly propose a learnable feature vector instead of a pre-selected one. More specifically, the feature vector is estimated from a reference set which contains all critical state-action pairs observed so far, and it can be updated by either adding a new pair or replace an existing one in the reference set. Thus the approximate Q-function keeps adjusting itself as the knowledge about the system accumulates via observations. Our algorithm is designed in both batch mode and online mode in the context of the traffic signal control. In addition, the convergence of this algorithm is ex-perimentally supported. Furthermore, some regularization methods are proposed to avoid overfitting of Q-function on the noisy observations. Finally, A simulation on the traffic signal control in a single intersection is provided, and the performance of this algorithm is compared with Q-learning, in which the Q-function is numerically estimated for each state-action pair without approximation.},
author = {Chu, Tianshu and Wang, Jie and Cao, Jian},
doi = {10.1109/CDC.2014.7039557},
isbn = {978-1-4673-6090-6},
journal = {53rd IEEE Conference on Decision and Control},
pages = {1277--1282},
title = {{Kernel-based reinforcement learning for traffic signal control with adaptive feature selection}},
year = {2014}
}
@incollection{Littman1994,
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly...},
author = {Littman, Michael L.},
booktitle = {Machine Learning Proceedings 1994},
doi = {10.1016/B978-1-55860-335-6.50027-1},
isbn = {1-55860-335-2},
issn = {00493848},
pages = {157--163},
pmid = {17034835},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Markov games as a framework for multi-agent reinforcement learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271},
year = {1994}
}
@article{DBLP:journals/corr/HeessTSLMWTEWER17,
archivePrefix = {arXiv},
arxivId = {1707.02286},
author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S M Ali and Riedmiller, Martin A and Silver, David},
eprint = {1707.02286},
journal = {CoRR},
title = {{Emergence of Locomotion Behaviours in Rich Environments}},
url = {http://arxiv.org/abs/1707.02286},
volume = {abs/1707.0},
year = {2017}
}
@article{Bernstein2002,
abstract = {We consider decentralized control of Markov decision processes and give complexity bounds on the worst-case running time for algorithms that find optimal solutions. Generalizations of both the fully observable case and the partially observable case that allow for decentralized control are described. For even two agents, the finite-horizon problems corresponding to both of these models are hard for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov decision processes. In contrast to the problems involving centralized control, the problems we consider provably do not admit polynomial-time algorithms. Furthermore, assuming EXP {\textless}= NEXP, the problems require superexponential time to solve in the worst case},
author = {Bernstein, D and Givan, R and Immerman, N},
doi = {10.1287/moor.27.4.819.297},
isbn = {1-55860-709-9},
issn = {0364-765X},
journal = {Mathematics of operations research},
number = {4},
pages = {819--840},
title = {{The complexity of decentralized control of Markov decision processes}},
url = {http://rbr.cs.umass.edu/papers/BGIZmor02.pdf},
volume = {27},
year = {2002}
}
@article{DBLP:journals/corr/AndrychowiczWRS17,
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {1707.01495},
journal = {CoRR},
title = {{Hindsight Experience Replay}},
url = {http://arxiv.org/abs/1707.01495},
volume = {abs/1707.0},
year = {2017}
}
@misc{Hinton2012,
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Swersky, Kevin},
booktitle = {COURSERA: Neural Networks for Machine Learning},
title = {{Lecture 6a Overview of mini-batch gradient descent}},
url = {http://www.cs.toronto.edu/{~}tijmen/csc321/slides/lecture{\_}slides{\_}lec6.pdf},
year = {2012}
}
@book{RichardS.SuttonandAndrewG.Barto2018,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core, online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new for the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
author = {{Richard S. Sutton and Andrew G. Barto}},
edition = {Second Edi},
isbn = {9780262039246},
pages = {550},
publisher = {MIT Press},
title = {{Reinforcement Learning, Second Edition An Introduction}},
year = {2018}
}
@article{Tahifa2015,
abstract = {Congestion, accidents, pollution, and many other problems resulting from urban traffic are present every day in most cities around the world. The growing number of traffic lights in intersections needs efficient control, and hence, automatic systems are essential nowadays for optimally tackling this task. Agent based technologies and reinforcements learning are largely used for modelling and controlling intelligent transportation systems, where agents represent a traffic signal controller. Each agent learns to achieve its goal through many episodes. With a complicated learning problem, it may take much computation time to acquire the optimal policy. In this paper, we use a population based methods such as particle swarm optimization to be able to find rapidly the global optimal solution for multi- modal functions with wide solution space. Agents learn through not only on their respective experiences, but also by exchanging information among them, simulation results show that the swarm Q-learning surpass the simple Q-learning causing less average delay time and higher flow rate.},
author = {Tahifa, Mohammed and Boumhidi, Jaouad and Yahyaouy, Ali},
doi = {10.1109/ISACV.2015.7105536},
isbn = {9781479975112},
journal = {Intelligent Systems and Computer Vision (ISCV), 2015},
pages = {1--6},
title = {{Swarm reinforcement learning for traffic signal control based on cooperative multi-agent framework}},
year = {2015}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann A. and Bengio, Yoshua and Hinton, Geoffrey E.},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@inproceedings{Fan2004,
abstract = {Based on the analysis of the reinforcement learning and Markov games, the paper proposes a layered multi-agent coordination framework Based on the multi-agent's interaction of competition and cooperation, this coordination framework adopts the zero-sum Markov game in higher layer to compete with opponent and adopts the team Markov game in lower layer to accomplish the team's cooperation. This coordination framework is applied to Robot Soccer. The results of the experiment illuminate that our proposed method is better than the traditional multi-agent learning. {\textcopyright} 2003 IEEE.},
author = {Fan, B. and Pan, Q. and Zhang, H.C.},
booktitle = {CSCWD 2004 - 8th International Conference on Computer Supported Cooperative Work in Design - Proceedings},
isbn = {0780379411},
title = {{A multi-agent coordination framework based on Markov games}},
volume = {2},
year = {2004}
}
@article{Liang2018,
abstract = {—Existing inefficient traffic light control causes nu-merous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. In terms of how to dynamically adjust traffic signals' duration, existing works either split the traffic signal into equal duration or extract limited traffic information from the real data. In this paper, we study how to decide the traffic signals' duration based on the collected data from different sensors and vehicular networks. We propose a deep reinforcement learning model to control the traffic light. In the model, we quantify the complex traffic scenario as states by collecting data and dividing the whole intersection into small grids. The timing changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map the states to rewards. The proposed model is composed of several components to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation in the Simulation of Urban MObility (SUMO) in a vehicular network, and the simulation results show the efficiency of our model in controlling traffic lights.},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.11115v1},
author = {Liang, Xiaoyuan and Du, Xusheng and Wang, Guiling and Han, Zhu},
eprint = {arXiv:1803.11115v1},
journal = {Ieee Transactions on Vehicular Technology},
keywords = {Index Terms—reinforcement learning,deep learning,traffic light control,vehicular network},
number = {Xx},
pages = {1--11},
title = {{Deep Reinforcement Learning for Traffic Light Control in Vehicular Networks}},
url = {https://arxiv.org/pdf/1803.11115.pdf},
volume = {1},
year = {2018}
}
@article{Tampuu2015,
abstract = {Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.},
archivePrefix = {arXiv},
arxivId = {1511.08779},
author = {Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
doi = {10.1371/journal.pone.0172395},
eprint = {1511.08779},
isbn = {1111111111},
issn = {1932-6203},
journal = {arXiv},
pages = {1--12},
pmid = {28380078},
title = {{Multiagent Cooperation and Competition with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.08779},
year = {2015}
}
@article{Papageorgiou1997,
abstract = {Asservissement Lin{\'{e}}aire d'Entr{\'{e}}e Autorouti{\`{e}}re (ALINEA), a local feedback ramp-metering strategy, has had multiple field applications, and more applications are planned in several European countries. The main features of ALINEA are presented and the field results achieved to date at both single and multiple ramps of the Boulevard P{\'{e}}riph{\'{e}}rique in Paris and at the A10 West motorway in Amsterdam are summarized. The reported results indicate easy application, flexibility, and high efficiency of ALINEA. Planned implementations are outlined.},
author = {Papageorgiou, M and Hadj-Salem, H. and Middelham, F.},
doi = {10.3141/1603-12},
isbn = {0361-1981},
issn = {0361-1981},
journal = {Transportation Research Record},
number = {970032},
pages = {90--98},
title = {{ALINEA local ramp metering: Summary of field results}},
volume = {1603},
year = {1997}
}
@misc{baselines,
author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
booktitle = {GitHub repository},
howpublished = {$\backslash$url{\{}https://github.com/openai/baselines{\}}},
publisher = {GitHub},
title = {{OpenAI Baselines}},
year = {2017}
}
@book{Puterman:1994:MDP:528623,
address = {New York, NY, USA},
author = {Puterman, Martin L},
edition = {1st},
isbn = {0471619779},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Markov Decision Processes: Discrete Stochastic Dynamic Programming}},
year = {1994}
}
@article{Liu2017CooperativeDR,
abstract = {Traffc signal control plays a crucial role in intelligent transportation system. However, the existing approaches for traffc signal control based on reinforcement learning mainly focus on traffc signal optimization for single intersection. We propose a deepreinforcement-learning-based approach to collaborative control traffc signal phases of multiple intersections. For the state representation, we use comprehensive road network information as input, and for each controller (intersection), suffcient autonomy is given so as to enable it to adapt to different kinds of intersections. Besides, we design a reward considering driver patience and cumulative delay, which can better reflect the reward from the road network. Based on the above opinions, a variant of independent deep Q-learning is introduced, so that multiple agents can be applied experience replay to speed up the training process.},
author = {Liu, Mengqi and Deng, Jiachuan and Xu, Ming and Zhang, Xianbo and Wang, Wei},
journal = {23rd ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), Halifax 2017},
title = {{Cooperative Deep Reinforcement Learning for Traffic Signal Control}},
year = {2017}
}
